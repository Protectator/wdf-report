%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               %
% ----- CONCLUSION PROJET ----- %
%                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion technique}

	Passons en revue les objectifs initiaux du projet afin d'évaluer les résultats obtenus. Reprenons donc les objectifs présentés au début du projet, au chapitre~\ref{objectifs} :

	\paragraph{Définir un profil utilisateur selon des critères de préférence, d’intérêt, d’habitude, d’opinion, etc.}

		Nous avons accompli la définition initiale du profil d'un utilisateur en lui demendant de choisir ses intérêts parmi une liste hiérarchisée d'une centaine de centres d'intérêts. Cet objectif a été accompli de manière simple en implémentant le choix d'intérêts parmi une liste adaptée que nous avons repris d'une source externe. Nous nous sommes donc intéressés particulièrement aux intérêts de l'utilisateur, et avons laissé de côté les aspects plus personnels comme ses opinions et orientations.

	\paragraph{Construire le profil d'un utilisateur en se basant sur sa navigation Internet ainsi que sur les métadonnées (durée de consultation des pages, heure de consultation, etc.). Des algorithmes de machine learning seront utilisés pour apprendre les profils en se basant sur des collections de profils annotées}

		Pour commencer, nous n'avons pas utilisé de collections de profils annotées. Nous avons crée notre propre collection de profils en implémentant un outil qui, en plus de récolter des informations sur l'utilisateur, lui demande certaines informations personelles, et nous permet d'évaluer et valider notre propre modèle de profils.

		La construction du profil de l'utilisateur se base en effet sur sa nagivation Internet, ainsi que sur certaines données supplémentaires de sa navigation : Le nombre de consultations des pages web, le temps d'activité de l'utilisateur sur les pages web visitées ainsi que le contenu publique des pages Web visitées.

		Nous utilisons plusieurs algorithmes de Machine Learning, dans la sous-catégorie de l'apprentissage automatique non supervisé. Nous faisons appel à un algorithme d'extraction de mots-clé (qui se nomme \gls{TF-IDF}) pour reconnaître les mots importants de chaque page web, et également un algorithme de topic modeling (nommé \gls{LDA}) dans le but d'apprendre les thèmes des pages visitées par l'utilisateur.
	
		Nous avons donc répondu à l'objectif initial, mais avons utilisé une méthodologie légèrement différente que celle prévue initialement. Cet objectif a pris plus d'importance que l'idée initiale, et avons en effet généré des résultats supplémentaires comme l'interface de visualisation destinée à l'utilisateur.

	\paragraph{Identifier des trackers qui ont la possibilité de construire des profiles utilisateurs en intégrant des données de plusieurs sources}

		Nous avons enregistré et identifié des trackers qui ont des possibilités de construction de profils, mais n'avons pas intégré d'autres types de données dans l'analyse. L'importance de cette tâche s'est réduite durant le projet au bénéfice de l'objectif précédent, qui a vu des résultats grandissants et un potentiel intéressant plus élevé.

		L'importance de cette partie s'est trouvée réduite durant le projet après une réévaluation des intérêts en jeu. L'objectif initial a donc été partiellement rempli, car celui-ci s'est trouvé aminci après un certain temps.

	\subsection{Réalisations}

		Au cours de ce projet, nous avons pu réaliser avec succès :
		\begin{itemize}
			\item Un état de l'art des techniques de tracking actuelles, ainsi que certaines méthodologies d'extraction de données permettant la génération de profils d'utilisateurs.
			\item Une solution complète de récolte, d'agrégation et d'analyse automatique de données de navigation web d'utilisateurs. Cette solution comprend également une interface permettant aux utilisateurs de consulter à tout moment les données que nous avons récoltées sur lui, ainsi que les plusieurs facettes du profil que nous avons généré à partir de ses données.
			\item Une analyse, alimentée par les données récoltées, révélant une parties des possibitlés de génération de profils d'utilisateurs en se basant sur leur navigation web.
			\item Une association dont le but est de sensibiliser le public à l'importance de la gestion de son identité et de ses traces digitales
		\end{itemize}

		L'ensemble de ces réalisations s'est articlé autour de la volonté de mettre en lumière le potentiel de détection de traits personnels de profils d'utilisateurs naviguant sur le Web. Chaque partie énoncée du projet contribue à sa manière à cet objectif.

\section{Travaux futurs}

	Les objectifs principaux du projet ont été atteints, mais les possibilités d'améliorations et de continuation du projet sont nombreuses et variées, à la fois sur le plan technique et sur le plan idéologique. Parmi ceux-ci, on peut citer principalement :
	\begin{itemize}
		\item \textbf{La distribution de l'extension à un public plus large}. Des optimisations dans l'implémentation ont permis au serveur actuel de supporter une base d'utilisateurs régulier bien plus large que celle qui a été utilisée pour cette étude. Sans changer d'architecture, projet est probablement capable de gérer une vingtaine d'utilisateurs concurrents sur la machine actuelle, soit probablement jusqu'à une centaine d'utilisateurs actifs totaux. Une idée peut donc être de développer un canal de communication pour toucher un public plus large, et ainsi amasser d'avantage de données. Une plus grande quantité de données va permettre d'améliorer l'évaluation des modèles actuels et d'accroître les révélations et les découvertes de trackers potentiels.
		\item \textbf{L'amélioration technique du nettoyage de données}. Actuellement, plusieurs techniques sont déjà mises en oeuvre pour éloigner les données indésirables dans notre pipeline, mais ces étapes sont insuffisantes pour éliminer encore une bonne partie d'informations qui biaisent les algorithmes. Ici, de nouvelles techniques pourraient être introduites à différents endroits de la pipeline. Parmi les probablement plus impactantes et plus faciles à mettre en place, on peut citer :
		\begin{itemize}
			\item L'amélioration de la liste d'URLs ignorées. Actuellement, une liste statique ignore certaines URL connues pour servir des données non voulues. Il serait possible d'améliorer grandement cette liste afin qu'elle contienne la plupart des sites web indésirables.
			\item L'amélioration de la détection d'activité de l'utilisateur. Actuellement, l'extension considère une page comme étant active si l'utilisateur a effectué une action dans les 30 dernières secondes avec le clavier ou la souris. Ceci ne prend pas en compte des autres actions comme la lecture d'une vidéo, ou l'écoute de musique par exemple. Une meilleure détection de l'intérêt de l'utilisateur pour une page serait bénéfique.
			\item L'amélioration de la détection des mots réellement affichés sur les pages web. La technique actuelle est d'émuler le lancement des URLs visitées dans un navigateur Chrome, et de récupérer le \gls{DOM} de la page puis d'en extraire le texte. Cette technique ne prend pas en compte si le texte présent dans les noeud DOM est effectivement affiché ou pas. Par exemple dans le cas d'une one-page app, il est fréquent que certains éléments soient simplement cachés de la page grâce à des directives CSS. Notre technique ne prend actuellement pas ceci en compte du tout, et conserve tous les mots présents dans les noeuds de la page.
			\item L'amélioration de la détection de langue sur les pages. La technique actuelle est de détecter une langue par page, puis de nettoyer les mots de cette page en ignorant les stopwords de cette langue. Cette technique a un point faible évident : Les pages qui contiennent du texte en plusieurs langues. Nous avons vu des exemples pratiques dans lesquels des pages multi-lingues ont donné des résultats aberrants lors de l'extraction de mots-clé, car les stopwords d'une ou plusieurs langues de la page n'ont pas été filtrés. Une amélioration pourrait être de lancer la détection la langue de la page par blocs de texte.
			\item Le stockage de plusieurs versions d'une page au cours du temps. Actuellement, seule la dernière version téléchargée d'une page fait foi pour tous les algorithmes. Ajouter la possibilité de conserver plusieurs copies d'une page dans le temps permettra de traiter correctement les données provenant de pages générées dynamiquement, par exemple les pages d'accueil de sites web de news.
		\end{itemize}
		\item \textbf{Automatisation du calcul offline}. Actuellement, les scripts offlines sont lancés ponctuellement par l'administrateur. Une automatisation du lancement de ces tâches réduirait le coût de maintenance du serveur grâce à un gain de temps.
		\item \textbf{Meilleure structures de données}. La structure de la base de données actuelle a été conçue pour un nombre d'utilisateurs réduit, ce qui convenait parfaitement pour le projet. Cependant si le nombre d'utilisateurs augmente, certaines structures de données et certains algorithmes devront être remaniés car ils ne scalent pas de manière suffisante. Par exemple, le pré-calcul de certaines informations commence déjà à prendre un temps considérable, de l'ordre d'environ une demi-heure.
		\item \textbf{Utilisations de modèles online}. Les deux modèles TF-IDF et LDA ne peuvent pas être mis à jour de manière online, ce qui signifie qu'il est nécessaire de lancer des exécutions ponctuelles de script périodiquement afin de rafraîchir certaines données de la base. Bien que ceci soit fonctionnel actuellement et que ces méthodes ont été choisies pour leur précision, il serait peut-être sage de se pencher vers des modèles pouvant être améliorés de manière continue afin de ne pas avoir à redémarrer le serveur.
		\item \textbf{Plusieurs méthodes de calcul des profils}. La méthode actuelle pour calculer les topics d'un utilisateur repose sur la multiplication intuitive des topics des pages de l'utilisateur avec le temps actif qu'il a passé dessus. Bien que fonctionelle, cette méthode n'est certainement pas la seule valable, et il serait très intéressant de tester d'autres hypothèses et méthodes pour la génération de topics d'un utilisateur. Par exemple, prendre plutôt le nombre de visites sur les pages au lieu du temps d'activité, ou alors de donner une importance non-linéaire au temps passé sur la page, par exemple exponentielle ou logarithmique. Des idées plus imaginatives sont également envisageables, comme par exemple assigner des poids différents aux mots contenus dans les pages web en fonction de la taille de leur police d'écriture.
		\item \textbf{Capture d'autres données utilisateurs} Bien que éthiquement discutable, il est toujours possible d'acquérir simplement plus de données différentes de l'utilisateur et de les prendre en compte dans la recherche. Par exemple, nous nous sommes volontairement limités dans cette étude dans le but de respecter au mieux les informations privées des utilisateurs. Nous avons par exemple exclu les paramètres de recherche dans une URL, alors qu'ils serait possible de les prendre en compte pour avoir des résultats plus précis. Il serait aussi envisageable d'envoyer simplement le contenu de chaque page visitée par le navigateur lui-même, au lieu d'accéder à une version publique de chaque document. Cette méthode, bien qu'elle arrangerait de nombreux problèmes énoncés précédamment, est évidamment très extrême car l'extensions dévoilerait alors absolument toute information affichée à l'écran de l'utilisateur.
	\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                    %
% ----- CONCLUSION PERSONNELLE ----- %
%                                    %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion personnelle}

	Mon choix pour ce projet s'est principalement fait car toutes les technologies du Web m'intéressent beacoup. Je suis avide d'apprendre des nouvelles compétences qui me permettent d'élargir mon horizon de connaissances sur les dernières technologies, autant pour les utiliser en développant des outils utiles, que pour réfléchir sur leurs possibilités et leurs risques. Le projet m'a grandement motivé du début à la fin car il s'inscrivait tout à fait dans cette ligne de pensée.
	J'ai pu développer à la fois mes compétences en matière de recherche et de mise en contexte des technologies actuelles, que d'innovation et de développement de nouveaux outils dans le but de sensibiliser les utilisateurs d'aujourd'hui à l'utilisation de certaines technologies. J'ai également grandement appris de l'analyse finale de données, qui n'était pas une tâche facile au vu de la nature très subjective et peu cartésienne des informations que j'ai dû traiter, du moins par rapport àce que j'ai lhabitude.
	Je suis d'avis que le grand public n'est généralement pas assez conscient des conséquences et des risques qu'ils prennent en publiant certaines informations en ligne, et j'ai l'impression d'avoir pu participer quelque peu à la sensibilisation globale en apportant ma modeste pierre à l'édifice grâce à ce projet. Loin d'être parfait, je suis tout de même très satisfait du résultat final qui me semble être un produit complet et adapté à l'objectif recherché en démarrant le projet.
	La nature open-source du code développé ici a également été très motivant dans tout le processus.
	Au final, je suis très content d'avoir eu l'occasion de travailler sur ce projet, qui m'a enrichi dans de nombreux aspects. 